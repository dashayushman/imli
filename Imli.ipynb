{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp_spacy_en = None\n",
    "nlp_spacy_es = None\n",
    "\n",
    "def get_spacy_model(lang=\"en\"):\n",
    "    global nlp_spacy_en\n",
    "    global nlp_spacy_es\n",
    "    if lang == \"en\":\n",
    "        if nlp_spacy_en is None: \n",
    "            nlp_spacy_en = spacy.load(lang)\n",
    "        return nlp_spacy_en\n",
    "    elif lang == \"es\":\n",
    "        if nlp_spacy_es is None: \n",
    "            nlp_spacy_es = spacy.load(lang)\n",
    "        return nlp_spacy_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def __init__(self, dataset_path, n_splits=3, ratio=0.3, augment=False):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.augment = augment\n",
    "        self.n_splits = n_splits\n",
    "        self.ratio = ratio\n",
    "        self.X, self.y = self.load()\n",
    "        self.splits = self.stratified_split(self.X, self.y, self.n_splits, self.ratio)\n",
    "    \n",
    "    def load(self):\n",
    "        with open(self.dataset_path, \"r\") as f:\n",
    "            dataset = json.load(f)\n",
    "            X = [sample[\"text\"] for sample in dataset[\"sentences\"]]\n",
    "            y = [sample[\"intent\"] for sample in dataset[\"sentences\"]]\n",
    "        return X, y\n",
    "    \n",
    "    def stratified_split(self, X, y, n_splits=10, test_size=0.2):\n",
    "        skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "        skf.get_n_splits(X, y)\n",
    "        splits = []\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            # print(\"TRAIN:\", train_index, \"\\n\\n\", \"TEST:\", test_index, \"\\n\\n\")\n",
    "            X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "            y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "            # add augmentation code here\n",
    "            splits.append({\"train\": {\"X\": X_train, \"y\": y_train},\n",
    "                           \"test\": {\"X\": X_test, \"y\": y_test}})\n",
    "        return splits\n",
    "    \n",
    "    def get_splits(self):\n",
    "        return self.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train ['Are there any hardware diagnostic tools?', 'Is there a tool like wifi analyzer for ubuntu?']\n",
      "y train ['Software Recommendation', 'Software Recommendation']\n",
      "X test ['How can I shutdown the computer when a certain process ends?', 'What are some good PHP editors?']\n",
      "y test ['Shutdown Computer', 'Software Recommendation']\n",
      "X train [\"What does my computer do when I click 'Shut Down'?\", 'Torrent client for the command-line?']\n",
      "y train ['Shutdown Computer', 'None']\n",
      "X test ['Cannot setup HP All in one DJ3630', 'How to record my screen?']\n",
      "y test ['Setup Printer', 'None']\n",
      "X train ['How is rm command different from the delete button?', 'Cannot install printer driver epson l210']\n",
      "y train ['None', 'Setup Printer']\n",
      "X test ['How to partially upgrade Ubuntu 11.10 from Ubuntu 11.04?', 'Is there any program for fuzzy string matching which provides a match score?']\n",
      "y test ['Make Update', 'Software Recommendation']\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(\"/home/dash/projects/imli/data/datasets/AskUbuntuCorpus.json\")\n",
    "splits = dataset.get_splits()\n",
    "for split in splits:\n",
    "    print(\"X train\", split[\"train\"][\"X\"][: 2])\n",
    "    print(\"y train\", split[\"train\"][\"y\"][:2])\n",
    "    print(\"X test\", split[\"test\"][\"X\"][: 2])\n",
    "    print(\"y test\", split[\"test\"][\"y\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "def semhash_tokenizer(text):\n",
    "    tokens = text.split(\" \")\n",
    "    final_tokens = []\n",
    "    for unhashed_token in tokens:\n",
    "        hashed_token = \"#{}#\".format(unhashed_token)\n",
    "        final_tokens += [''.join(gram)\n",
    "                         for gram in list(find_ngrams(list(hashed_token), 3))]\n",
    "    return final_tokens\n",
    "\n",
    "class SemhashFeaturizer:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = self.get_vectorizer()\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False,\n",
    "                               sublinear_tf=True, tokenizer=semhash_tokenizer)\n",
    "    \n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        self.vectorizer.fit(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.4472136  0.4472136  0.4472136  0.4472136  0.        ]\n",
      " [0.37796447 0.37796447 0.37796447 0.         0.37796447 0.37796447\n",
      "  0.37796447 0.         0.         0.         0.         0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "X, y = [\"hello\", \"I am a boy\"], [\"A\", \"B\"]\n",
    "\n",
    "semhash_featurizer = SemhashFeaturizer()\n",
    "semhash_featurizer.fit(X, y)\n",
    "X_ = semhash_featurizer.transform(X)\n",
    "print(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VFeaturizer:\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "    \n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return np.array([get_spacy_model(self.lang)(s).vector for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.47970939e+00 -4.21159118e-01  1.06445110e+00 -7.18615353e-02\n",
      "   2.12101126e+00  1.39717579e+00 -2.16116333e+00  1.11346400e+00\n",
      "   7.06262469e-01  1.18206465e+00  2.39653468e+00 -1.42033482e+00\n",
      "   7.76870430e-01 -2.62151456e+00 -2.49098325e+00  1.40702152e+00\n",
      "  -4.85620677e-01  3.30704629e-01 -1.01403928e+00  2.55613375e+00\n",
      "   5.65430641e-01  3.04582620e+00  3.22912407e+00 -8.35585117e-01\n",
      "  -1.43064380e+00 -7.87177205e-01 -1.76929116e-01  2.46902883e-01\n",
      "   1.35566425e+00 -2.21005678e-02  1.12495184e+00  7.40879893e-01\n",
      "  -7.66252279e-01  1.47947860e+00 -7.61319637e-01  1.02593696e+00\n",
      "  -2.20128202e+00  2.84820342e+00 -1.94958615e+00 -3.10578346e-02\n",
      "   4.57328707e-02  4.47483063e+00  1.55972350e+00  1.20723069e-01\n",
      "  -3.10773039e+00 -6.38728380e-01 -3.72742558e+00 -2.00596333e-01\n",
      "   3.07094789e+00 -2.38101530e+00 -1.00460863e+00 -9.03264642e-01\n",
      "  -6.01312101e-01 -9.85089958e-01 -2.94777364e-01  6.18771017e-01\n",
      "   3.57110262e+00 -2.22199011e+00 -3.35746980e+00 -8.80977333e-01\n",
      "  -1.39037991e+00 -2.17774302e-01 -3.89755917e+00  1.77484989e+00\n",
      "  -3.57757020e+00 -6.75826311e-01  2.38380718e+00 -2.74889064e+00\n",
      "   1.16637850e+00  3.30840850e+00 -7.55106747e-01 -4.48238492e-01\n",
      "   6.02241874e-01 -8.79089236e-01 -1.24272108e+00 -9.77721870e-01\n",
      "   5.06379032e+00 -2.49255490e+00 -4.29226875e+00 -1.50517058e+00\n",
      "  -3.11670876e+00 -3.85990930e+00 -8.26970637e-01  1.82351470e-01\n",
      "  -1.33350134e-01  1.39964700e-01  5.24794400e-01 -2.65564275e+00\n",
      "  -2.91033316e+00  1.58544230e+00 -4.13616943e+00  6.09565163e+00\n",
      "  -1.22401094e+00  8.80024552e-01 -1.92105162e+00  2.92261314e+00\n",
      "  -6.80996180e-02  1.72682595e+00  8.78647029e-01  1.69196641e+00\n",
      "  -4.70224917e-01 -2.20260739e-01 -2.16628027e+00 -2.92678571e+00\n",
      "   2.38116407e+00 -3.35327625e-01  1.32987034e+00 -4.27120447e+00\n",
      "  -8.30731571e-01  5.01394129e+00 -9.49329197e-01  2.14265704e+00\n",
      "   1.86848569e+00 -6.98740542e-01  3.45580506e+00  7.84632206e-01\n",
      "  -7.96630681e-01  4.18009758e-02  1.66403711e-01 -2.24284828e-01\n",
      "   3.85045934e+00  5.80369771e-01  4.28010106e-01 -4.97605413e-01\n",
      "  -2.47904122e-01  1.16080713e+00  2.83814526e+00  2.58258700e-01\n",
      "  -3.57589692e-01  4.53740001e-01  7.29589462e-02  6.53671995e-02\n",
      "  -6.08952165e-01  2.57639199e-01 -1.23331100e-02  6.06452823e-01\n",
      "   1.66445181e-01  7.73323923e-02  1.62329286e-01  8.17605793e-01\n",
      "  -5.34380376e-01 -2.02439696e-01 -2.49151394e-01 -3.17176729e-01\n",
      "  -3.85002375e-01  6.64706528e-02 -3.81279618e-01  2.79117227e-02\n",
      "  -1.75087333e-01  6.01196289e-01 -2.70908386e-01  3.47608864e-01\n",
      "  -2.29480267e-01 -3.98598254e-01 -1.01566315e-04 -1.77654877e-01\n",
      "  -4.44862932e-01 -1.23003602e+00  3.41191202e-01 -2.94646680e-01\n",
      "   2.85768509e-02 -4.81482983e-01 -1.06395990e-01 -2.15405703e-01\n",
      "   3.63263071e-01  4.95413765e-02  8.63414884e-01  3.57996881e-01\n",
      "  -8.18842113e-01 -1.72112852e-01  7.79591501e-01  1.02694824e-01\n",
      "   3.90399545e-01  5.95805645e-01  5.68966746e-01  2.71205008e-02\n",
      "  -6.21240854e-01 -7.80854821e-02  9.73114371e-02 -4.22537364e-02\n",
      "  -3.85772705e-01  1.12106204e-01  7.55585730e-02 -1.42952800e-02\n",
      "  -8.38410854e-03  1.96753681e-01  1.13068414e+00 -6.66878104e-01\n",
      "   7.27036953e-01  8.83777320e-01 -1.72820598e-01 -2.05869526e-02\n",
      "  -3.07386011e-01  2.28301868e-01  1.90745384e-01 -5.11765480e-01\n",
      "   1.64342672e-02 -2.40174979e-02 -4.25079286e-01 -2.04028189e-01\n",
      "   3.87338281e-01  2.88638860e-01  8.09488446e-02  5.41900039e-01\n",
      "   8.30960423e-02  1.76208794e-01  2.14940578e-01 -1.41054690e-02\n",
      "  -3.47544670e-01 -3.87998641e-01 -7.51093030e-01 -5.96484393e-02\n",
      "  -1.72324687e-01  3.18997800e-01 -1.04521549e+00 -4.06192482e-01\n",
      "  -6.14567518e-01  2.86515623e-01 -1.84958547e-01  2.17649847e-01\n",
      "  -4.76594985e-01 -5.37476003e-01 -7.36374557e-02 -1.16931118e-01\n",
      "  -2.60534793e-01 -3.45160693e-01 -2.35879928e-01 -1.31801903e-01\n",
      "  -1.08716309e-01  7.48786449e-01 -5.89273930e-01  6.51698351e-01\n",
      "   4.11379308e-01  3.48357767e-01 -5.05289972e-01  2.22297907e-02\n",
      "  -3.42448413e-01 -4.23920423e-01 -1.56041354e-01  1.45032704e-01\n",
      "   4.05257463e-01 -7.64677882e-01  7.67264605e-01 -7.88081169e-01\n",
      "  -4.64854658e-01  1.16343230e-01 -1.91479862e-01  6.54743195e-01\n",
      "  -6.92641437e-01 -8.06225091e-02  1.87881961e-01  3.51274759e-01\n",
      "   1.43542454e-01  7.85099030e-01  3.10091749e-02  1.13336587e+00\n",
      "  -3.70266289e-01 -5.68817556e-01  5.14196754e-01  3.33423257e-01\n",
      "  -4.99008298e-01 -2.63836712e-01 -2.04448104e-01 -1.24037170e+00\n",
      "  -8.34260136e-03 -5.27354300e-01  2.11079344e-01  2.76667103e-02\n",
      "   1.99555576e-01 -4.69241410e-01 -2.44299043e-02  6.54638052e-01\n",
      "  -7.26376772e-01 -3.75816971e-02  9.06417295e-02 -6.04587138e-01\n",
      "  -3.25310737e-01 -2.28741109e-01  5.26296973e-01 -2.57758796e-01\n",
      "  -7.55272150e-01  3.54329497e-01 -5.93653440e-01  3.07720512e-01\n",
      "   4.59971040e-01 -2.24935502e-01 -7.71758199e-01  3.76646399e-01\n",
      "  -2.88754463e-01  8.31215203e-01  1.85737419e+00 -5.88413700e-02\n",
      "  -4.92537022e-02  7.80452847e-01  4.00582105e-02  1.53204933e-01\n",
      "  -5.57155192e-01  4.25419807e-02  2.42131755e-01 -3.70211184e-01\n",
      "   6.70264065e-01  2.30553746e-03  6.65956676e-01 -4.25681889e-01\n",
      "  -2.11215466e-01 -3.78016084e-01 -3.69925946e-01 -2.61583328e-01\n",
      "  -3.82034257e-02  1.42881989e-01  4.87169623e-02 -1.14144146e-01\n",
      "  -2.20856041e-01 -4.80841368e-01  1.10110290e-01  3.92951608e-01\n",
      "   5.02271295e-01  1.26558125e+00 -4.25443649e-01 -7.46932477e-02\n",
      "   6.82308376e-02  7.42180407e-01 -4.44720209e-01  4.67977077e-01\n",
      "  -2.00424567e-02  6.47412598e-01 -8.90937030e-01 -6.34675562e-01\n",
      "   3.45851243e-01  3.12506497e-01  1.34863389e+00  2.40537629e-01\n",
      "  -1.11101151e-01  1.01044483e-01 -2.00340807e-01 -2.73435026e-01\n",
      "  -7.16401875e-01 -4.27165121e-01 -6.56185746e-02 -3.95269066e-01\n",
      "  -1.60569578e-01 -6.47916496e-02  8.07901621e-02 -6.13796189e-02\n",
      "   4.27913100e-01 -1.79543734e-01  7.45757997e-01  1.03759491e+00\n",
      "  -4.07631129e-01  9.27716941e-02 -1.21910088e-01  1.87927723e-01\n",
      "  -6.58795536e-01  8.01662505e-01  1.31530017e-02  1.39450327e-01\n",
      "  -2.50883043e-01 -2.21554130e-01 -6.75984800e-01 -1.01418287e-01\n",
      "  -3.66055608e-01 -4.10327390e-02  7.34927729e-02  6.81622028e-02\n",
      "  -1.09200156e+00  7.34210372e-01 -3.97188067e-01 -1.37627006e-01\n",
      "   6.90972388e-01  5.06674349e-01  2.88543910e-01  1.18510865e-01\n",
      "   6.16600692e-01 -6.18702888e-01 -1.87203556e-01  1.03379261e+00\n",
      "  -7.38960952e-02 -1.34925276e-01 -3.18909436e-02 -1.26902983e-01\n",
      "  -4.50662613e-01 -3.74536663e-01  1.56267490e-02  1.68377876e-01]\n",
      " [-1.07358694e-01 -1.33081451e-01  2.43980646e+00  3.40239453e+00\n",
      "   2.90284538e+00 -1.68872976e+00 -8.14951420e-01 -7.48299658e-01\n",
      "   3.35011780e-02  1.24830747e+00  1.40609598e+00 -4.72951382e-01\n",
      "   1.00103760e+00  7.13223219e-02 -1.45844817e+00 -9.68565702e-01\n",
      "  -9.35006142e-01  1.81470454e-01  4.03159499e-01  1.06141075e-01\n",
      "   1.52371907e+00 -5.46328664e-01 -8.03680897e-01  5.35612643e-01\n",
      "  -8.20081234e-01 -1.14429855e+00 -9.31734920e-01 -6.64546490e-01\n",
      "   3.74585509e-01 -1.62471581e+00 -2.72442967e-01  8.46104920e-01\n",
      "  -1.60467863e+00  6.66049719e-01  6.91861749e-01 -1.55554819e+00\n",
      "  -3.11811984e-01 -7.27541327e-01 -1.26444697e-01  5.16533613e-01\n",
      "  -1.20326185e+00  1.02713466e+00  2.87100494e-01 -5.78603625e-01\n",
      "  -1.04136181e+00 -9.02005672e-01  6.90839827e-01 -2.62466371e-01\n",
      "   1.55821621e-01 -6.61885500e-01  9.97537971e-01 -1.99619365e+00\n",
      "  -1.00042605e+00 -1.84121609e-01  1.46643662e+00  1.59433270e+00\n",
      "   6.67308390e-01  2.09772706e+00 -1.66049457e+00  1.93661344e+00\n",
      "  -1.05084038e+00  9.03002977e-01  1.72857478e-01  1.30317056e+00\n",
      "  -1.47116911e+00  1.40588367e+00 -1.24969935e+00 -3.84584546e-01\n",
      "  -4.81232107e-01 -5.32016098e-01  5.63477874e-01 -1.44677711e+00\n",
      "   2.41312742e-01  1.36143804e+00 -1.73379767e+00 -6.61131501e-01\n",
      "   1.71848965e+00  3.71301740e-01 -1.43329859e-01  3.83291930e-01\n",
      "  -1.49883974e+00 -1.42904580e+00  2.04317689e-01 -1.34513927e+00\n",
      "   7.79212832e-01 -1.37066400e+00 -7.84978271e-02  1.48377502e+00\n",
      "  -1.38204277e+00  4.99823153e-01 -1.89927459e+00  1.40836632e+00\n",
      "  -4.02878284e-01  2.51333284e+00 -1.67665291e+00  3.18827450e-01\n",
      "  -1.26091480e+00  5.10744631e-01  1.66771293e-01 -2.02630734e+00\n",
      "  -5.05718768e-01  5.41481376e-02 -3.65998149e-01 -9.76581037e-01\n",
      "   1.33196926e+00 -8.83346200e-02  1.03198743e+00 -1.57705188e+00\n",
      "  -1.07041799e-01 -5.98121405e-01  1.52186656e+00  2.70838320e-01\n",
      "   7.21721888e-01  8.53713453e-01  2.09798050e+00  5.60394645e-01\n",
      "  -4.61695999e-01  3.36050928e-01 -4.40223575e-01 -5.46452582e-01\n",
      "   3.29230714e+00 -5.42972863e-01  2.14157268e-01  2.40379739e+00\n",
      "   1.56158328e+00  2.53248394e-01  3.95082057e-01 -1.67783189e+00\n",
      "  -3.18546951e-01 -4.18797731e-01 -7.98995793e-02  2.63713270e-01\n",
      "   3.17195058e-02 -2.33848423e-01 -5.06392837e-01  3.39366972e-01\n",
      "   3.32886130e-01 -1.19369529e-01 -2.03779712e-01 -3.03672433e-01\n",
      "   4.21156287e-01 -2.04742417e-01 -2.95467198e-01  3.79433900e-01\n",
      "   3.46028879e-02  1.82810612e-02 -5.31684607e-04 -1.54169016e-02\n",
      "  -1.14571840e-01  2.40889668e-01 -6.63462803e-02  5.22213839e-02\n",
      "  -5.11036813e-03 -2.71070823e-02 -2.89169326e-03 -1.32472843e-01\n",
      "   3.26959938e-02 -6.02014184e-01  3.11134979e-02 -3.04738507e-02\n",
      "  -6.03819713e-02 -2.43021473e-01 -1.62242651e-01  1.34466603e-01\n",
      "   2.22876877e-01 -2.04547256e-01  3.71639818e-01  8.00207257e-02\n",
      "  -2.30028152e-01  2.97689855e-01  6.31595552e-02  3.12259346e-02\n",
      "   1.16535947e-01 -2.62930989e-03  4.58526164e-01  1.00729108e-01\n",
      "  -3.72566938e-01 -2.47720070e-02 -2.58501321e-01  5.51470146e-02\n",
      "  -2.39489734e-01 -8.41401666e-02 -1.15607426e-01  2.91111588e-01\n",
      "   1.65256485e-02 -2.70146370e-01  4.23502684e-01 -1.02528162e-01\n",
      "   2.08953246e-01  1.73919365e-01  1.41755983e-01  2.55050868e-01\n",
      "   1.06352143e-01  9.06458795e-02 -1.42725497e-01  2.11888999e-01\n",
      "  -3.17741811e-01  2.82160580e-01 -2.18952194e-01 -3.88645232e-01\n",
      "   7.05423132e-02  2.93622017e-01  2.07224041e-01  2.01925725e-01\n",
      "   3.10128331e-01  1.94532961e-01  2.62086362e-01 -1.01919606e-01\n",
      "  -3.43504161e-01  1.50556371e-01 -1.63169235e-01 -3.49368528e-03\n",
      "   2.83898190e-02 -2.42078394e-01 -4.06607985e-01 -9.25980881e-02\n",
      "  -1.45482928e-01 -4.27711084e-02 -2.80787379e-01 -5.00386655e-01\n",
      "  -3.87254916e-02 -3.39441448e-01  3.26971114e-01  3.21231812e-01\n",
      "  -1.95513099e-01  1.45680785e-01  2.22826190e-02 -2.81539798e-01\n",
      "   2.19816014e-01  2.54976481e-01  5.62080562e-01  3.59794140e-01\n",
      "   2.48742998e-01  1.84140019e-02  4.72040921e-02  1.93782672e-02\n",
      "  -1.66246042e-01 -1.97046936e-01 -2.55295932e-01 -1.47421002e-01\n",
      "   1.55757919e-01 -1.78466022e-01 -1.46271259e-01 -4.59928066e-01\n",
      "   7.61535019e-03  1.04095876e-01  6.87567592e-02  5.80983981e-03\n",
      "   2.07895979e-01 -1.11409962e-01 -3.98211293e-02  5.25642112e-02\n",
      "   5.59895709e-02 -7.65344054e-02  2.18611747e-01  5.17881453e-01\n",
      "   2.70568430e-02 -2.40552083e-01  2.86673546e-01 -2.90321633e-02\n",
      "  -2.52396256e-01 -1.54589266e-01 -4.73183692e-02  2.17442624e-02\n",
      "  -6.63960353e-02 -1.18710130e-01 -1.08745135e-01 -5.33761799e-01\n",
      "   1.51664689e-01 -5.77158481e-02 -1.51391953e-01  1.20800145e-01\n",
      "   2.39540040e-01 -1.08118720e-01 -1.60283282e-01 -2.93290138e-01\n",
      "   5.10007665e-02 -1.25291929e-01  5.97610414e-01 -5.48652336e-02\n",
      "   5.22793233e-02  2.05332160e-01 -2.18069658e-01 -2.03247555e-02\n",
      "  -1.14394456e-01  3.28556448e-03 -4.77285028e-01 -4.46029991e-01\n",
      "  -7.81627074e-02 -1.63030773e-02  1.94619849e-01 -1.43316165e-01\n",
      "   5.67506030e-02  3.80875379e-01  2.50892788e-01 -6.61431178e-02\n",
      "   6.55879974e-02  1.95429996e-01 -1.94613874e-01 -2.65066624e-01\n",
      "   1.81961134e-01  4.18056920e-02  1.83047503e-02 -3.04794610e-01\n",
      "   2.37940580e-01 -2.80263603e-01  1.23475254e-01 -4.28009927e-01\n",
      "  -2.61047482e-01 -2.31424570e-02 -4.03467685e-01  3.52160007e-01\n",
      "  -3.85328382e-01 -4.65865582e-02  3.06179494e-01 -1.79691404e-01\n",
      "  -3.38142067e-02  4.29680884e-01 -1.93320602e-01  6.40580803e-02\n",
      "  -2.62805820e-02  4.81086671e-01 -1.20306507e-01  4.45885658e-01\n",
      "  -2.02196181e-01  2.77557582e-01  3.57967839e-02  5.79681434e-02\n",
      "   2.30379760e-01 -1.88480198e-01 -3.00668515e-02 -1.90290958e-02\n",
      "  -1.82427943e-01  4.52080041e-01 -1.12600051e-01 -1.40458107e-01\n",
      "  -1.26616985e-01 -3.20791937e-02 -1.99921310e-01 -1.39407262e-01\n",
      "   1.81545526e-01  1.73449606e-01  3.13782215e-01 -7.78345317e-02\n",
      "  -8.06628168e-02 -1.68963358e-01  1.13921314e-02  9.12649333e-02\n",
      "   2.74258882e-01 -1.93996951e-01 -3.66978317e-01  7.54955411e-02\n",
      "  -4.38554704e-01 -2.55809352e-03  6.09610155e-02 -1.76633224e-02\n",
      "  -5.60114503e-01 -2.35521555e-01 -6.53685480e-02 -5.28644383e-01\n",
      "  -3.14765364e-01 -1.02234051e-01 -7.27175772e-02 -4.80207205e-02\n",
      "  -5.42016685e-01 -2.77820855e-01  3.51771832e-01 -2.10275427e-01\n",
      "   4.06561457e-02  3.20782512e-01 -3.75766903e-01  2.05490649e-01\n",
      "  -1.83572963e-01 -1.73511416e-01  6.28203213e-01  1.24340676e-01\n",
      "   2.32040454e-02 -1.11118749e-01  2.82640755e-01  4.45047438e-01\n",
      "   6.42407089e-02  2.60978639e-02  1.82026148e-01  2.24935859e-01]]\n"
     ]
    }
   ],
   "source": [
    "X, y = [\"hello\", \"I am a boy\"], [\"A\", \"B\"]\n",
    "glove_path = \"\"\n",
    "w2v_featurizer = W2VFeaturizer(\"en\")\n",
    "w2v_featurizer.fit(X, y)\n",
    "X_ = w2v_featurizer.transform(X)\n",
    "print(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, splits, featurizer, path=\"data/plots\", lang=\"en\", name=\"default\"):\n",
    "        self.path = os.path.join(path, name)\n",
    "        if not os.path.exists(self.path): os.makedirs(self.path)\n",
    "        self.splits = splits\n",
    "        self.featurizer = featurizer\n",
    "        self.lang = lang\n",
    "        self.results = None\n",
    "    \n",
    "    def get_X_andy_from_split(self, split):\n",
    "        train_corpus, y_train = split[\"train\"][\"X\"], split[\"train\"][\"y\"]\n",
    "        test_corpus, y_test = split[\"test\"][\"X\"], split[\"test\"][\"y\"]\n",
    "        self.featurizer.fit(train_corpus)\n",
    "        self.featurizer.fit(test_corpus)\n",
    "        X_train = self.featurizer.transform(train_corpus)\n",
    "        X_test = self.featurizer.transform(test_corpus)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        parameters_mlp={'hidden_layer_sizes':[(100,50),(300,100,50),(200,100)]}\n",
    "        parameters_RF={ \"n_estimators\" : [50,60,70], \"min_samples_leaf\" : [1, 2]}\n",
    "        k_range = list(range(1, 11))\n",
    "        parameters_knn = {'n_neighbors':k_range}\n",
    "        \n",
    "        for i_s, split in enumerate(self.splits):\n",
    "            print(\"Evaluating Split {}\".format(i_s))\n",
    "            X_train, y_train, X_test, y_test = self.get_X_andy_from_split(split)\n",
    "            X_train = self.featurizer.fit(X_train)\n",
    "            X_test = self.featurizer.fit(X_test)\n",
    "            print(\"Train Size: {}\\nTest Size: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "            results = []\n",
    "            #alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "            knn=KNeighborsClassifier(n_neighbors=5)\n",
    "            for clf, name in [  \n",
    "                    (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "                    (GridSearchCV(knn,parameters_knn, cv=10),\"gridsearchknn\"),\n",
    "                    (GridSearchCV(MLPClassifier(activation='tanh'),parameters_mlp, cv=10),\"gridsearchmlp\"),\n",
    "                    (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "                    (GridSearchCV(RandomForestClassifier(n_estimators=10),parameters_RF, cv=10),\"gridsearchRF\")]:\n",
    "                print('=' * 80)\n",
    "                print(name)\n",
    "                results.append(benchmark(clf, X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "            for penalty in [\"l2\", \"l1\"]:\n",
    "                print('=' * 80)\n",
    "                print(\"%s penalty\" % penalty.upper())\n",
    "                # Train Liblinear model\n",
    "                #grid=(GridSearchCV(LinearSVC,parameters_Linearsvc, cv=10),\"gridsearchSVC\")\n",
    "                #results.append(benchmark(LinearSVC(penalty=penalty), X_train, y_train, X_test, y_test, target_names,\n",
    "                                        # feature_names=feature_names))\n",
    "                results.append(benchmark(LinearSVC(penalty=penalty, dual=False,tol=1e-3),\n",
    "                                         X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "                # Train SGD model\n",
    "                results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                                       penalty=penalty),\n",
    "                                         X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "            # Train SGD with Elastic Net penalty\n",
    "            print('=' * 80)\n",
    "            print(\"Elastic-Net penalty\")\n",
    "            results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                                   penalty=\"elasticnet\"),\n",
    "                                     X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "            # Train NearestCentroid without threshold\n",
    "            print('=' * 80)\n",
    "            print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "            results.append(benchmark(NearestCentroid(),\n",
    "                                     X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "            # Train sparse Naive Bayes classifiers\n",
    "            print('=' * 80)\n",
    "            print(\"Naive Bayes\")\n",
    "            results.append(benchmark(MultinomialNB(alpha=.01),\n",
    "                                     X_train, y_train, X_test, y_test, target_names))\n",
    "            results.append(benchmark(BernoulliNB(alpha=.01),\n",
    "                                     X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "            print('=' * 80)\n",
    "            print(\"LinearSVC with L1-based feature selection\")\n",
    "            # The smaller C, the stronger the regularization.\n",
    "            # The more regularization, the more sparsity.\n",
    "\n",
    "\n",
    "            results.append(benchmark(Pipeline([\n",
    "                                          ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                                                          tol=1e-3))),\n",
    "                                          ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "                                     X_train, y_train, X_test, y_test, target_names))\n",
    "           # print(grid.grid_scores_)\n",
    "           #KMeans clustering algorithm \n",
    "            print('=' * 80)\n",
    "            print(\"KMeans\")\n",
    "            results.append(benchmark(KMeans(n_clusters=2, init='k-means++', max_iter=300,\n",
    "                        verbose=0, random_state=0, tol=1e-4),\n",
    "                                     X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "\n",
    "\n",
    "            print('=' * 80)\n",
    "            print(\"LogisticRegression\")\n",
    "            #kfold = model_selection.KFold(n_splits=2, random_state=0)\n",
    "            #model = LinearDiscriminantAnalysis()\n",
    "            results.append(benchmark(LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
    "                  fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "                  multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "                  solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
    "                                     X_train, y_train, X_test, y_test, target_names))\n",
    "\n",
    "            plot_results(results)\n",
    "    \n",
    "    \n",
    "    def benchmark(clf, X_train, y_train, X_test, y_test, target_names,\n",
    "              print_report=True, print_top10=False,\n",
    "              print_cm=True):\n",
    "        print('_' * 80)\n",
    "        print(\"Training: \")\n",
    "        print(clf)\n",
    "        t0 = time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_time = time() - t0\n",
    "        print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "        t0 = time()\n",
    "        pred = clf.predict(X_test)\n",
    "        test_time = time() - t0\n",
    "        print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "        score = metrics.accuracy_score(y_test, pred)\n",
    "        print(\"accuracy:   %0.3f\" % score)\n",
    "        #print(\"Accuracy: %0.3f (+/- %0.3f)\" % (score.mean(), score.std() * 2))\n",
    "\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "            print(\"density: %f\" % density(clf.coef_))\n",
    "            print()\n",
    "\n",
    "        if print_report:\n",
    "            print(\"classification report:\")\n",
    "            print(metrics.classification_report(y_test, pred,\n",
    "                                                target_names=target_names))\n",
    "\n",
    "        if print_cm:\n",
    "            print(\"confusion matrix:\")\n",
    "            print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "        print()\n",
    "        clf_descr = str(clf).split('(')[0]\n",
    "        return clf_descr, score, train_time, test_time\n",
    "    \n",
    "    def plot_results(results):\n",
    "        # make some plots\n",
    "        indices = np.arange(len(results))\n",
    "\n",
    "        results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "        clf_names, score, training_time, test_time = results\n",
    "        training_time = np.array(training_time) / np.max(training_time)\n",
    "        test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(\"Score\")\n",
    "        plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "        plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "                 color='c')\n",
    "        plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "        plt.yticks(())\n",
    "        plt.legend(loc='best')\n",
    "        plt.subplots_adjust(left=.25)\n",
    "        plt.subplots_adjust(top=.95)\n",
    "        plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "        for i, c in zip(indices, clf_names):\n",
    "            plt.text(-.3, i, c)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Split 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-7450fb06a7d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                   name=\"Ubuntu\")\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-111-caf1ebd5fbdf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating Split {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_X_andy_from_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Size: {}\\nTest Size: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-b3e9ddb6afc5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/chai/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/chai/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/chai/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/chai/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/chai/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "semhash_featurizer = SemhashFeaturizer()\n",
    "dataset = Dataset(\"/home/dash/projects/imli/data/datasets/AskUbuntuCorpus.json\")\n",
    "splits = dataset.get_splits()\n",
    "\n",
    "trainer = Trainer(splits, semhash_featurizer, lang=\"en\", path=\"/home/dash/projects/imli/data/plots\", \n",
    "                  name=\"Ubuntu\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}